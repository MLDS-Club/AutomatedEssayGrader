{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Import other functions\n",
    "from process.averageWordLength import averageWordLength\n",
    "from process.misspellings import nmisspelled\n",
    "from process.wordcount import length\n",
    "from process.averageSentenceLength import averageSentenceLength\n",
    "from process.grammarchecker import grammarCheck\n",
    "from process.keywords import keyWords\n",
    "from process.sentcount import sentcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean essays\n",
    "df = pd.read_feather('data/essays.feather')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['cleaned_essay'] = df['essay'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing filler words\n",
    "def filler(word):\n",
    "    if (word.isupper() == True) and (any([char.isdigit() for char in word])):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "df['cleaned_essay2'] = df['cleaned_essay'].apply(lambda i : [x for x in i if not filler(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0        [Dear, local, newspaper, think, effects, compu...\n",
       "1        [Dear, believe, using, computers, benefit, us,...\n",
       "2        [Dear, people, use, computers, everyone, agree...\n",
       "3        [Dear, Local, Newspaper, found, many, experts,...\n",
       "4        [Dear, know, computers, positive, effect, peop...\n",
       "                               ...                        \n",
       "12971    [stories, mothers, daughters, either, enemies,...\n",
       "12972    [never, understood, meaning, laughter, shortes...\n",
       "12973    [laugh, habit, cause, causes, laughing, even, ...\n",
       "12974    [Trippin, fences, years, young, short, years, ...\n",
       "12975    [Many, people, believe, laughter, improve, lif...\n",
       "Name: cleaned_essay2_no_sw, Length: 12976, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['cleaned_essay2_no_sw'] = df['cleaned_essay2'].apply(lambda x : [w for w in x if not w.lower() in stop_words])\n",
    "\n",
    "df['cleaned_essay2_no_sw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['essay_id', 'essay_set', 'essay', 'rater1_domain1', 'rater2_domain1',\n",
       "       'rater3_domain1', 'domain1_score', 'rater1_domain2', 'rater2_domain2',\n",
       "       'domain2_score', 'rater1_trait1', 'rater1_trait2', 'rater1_trait3',\n",
       "       'rater1_trait4', 'rater1_trait5', 'rater1_trait6', 'rater2_trait1',\n",
       "       'rater2_trait2', 'rater2_trait3', 'rater2_trait4', 'rater2_trait5',\n",
       "       'rater2_trait6', 'rater3_trait1', 'rater3_trait2', 'rater3_trait3',\n",
       "       'rater3_trait4', 'rater3_trait5', 'rater3_trait6', 'cleaned_essay',\n",
       "       'cleaned_essay_no_sw'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df = pd.read_feather('~/Documents/GitHub/AutomatedEssayGrader/essaygrader/data/essays.feather')\n",
    "df = df.drop('Average Word Length', axis=1)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather('~/Documents/GitHub/AutomatedEssayGrader/essaygrader/data/essays.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making new features\n",
    "df['Average Word Length'] = df['cleaned_essay2'].apply(averageWordLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['misspelled'] = df['cleaned_essay2'].apply(nmisspelled)"
   ]
  },
  {
   "source": [
    "def length(essay):\n",
    "    return len(essay)\n",
    "df['word_count'] = df['cleaned_essay2'].apply(length)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_sentence_length'] = df['essay'].apply(averageSentenceLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------Need to find a faster grammar checker-------\n",
    "# df['grammar_errors'] = df['essay'].apply(grammarCheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0        67.0\n",
       "1        75.0\n",
       "2        58.0\n",
       "3        83.0\n",
       "4        67.0\n",
       "         ... \n",
       "12971    58.0\n",
       "12972    53.0\n",
       "12973    67.0\n",
       "12974    67.0\n",
       "12975    67.0\n",
       "Name: normalized_score, Length: 12976, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# Setting the prompts and normalizing the scores\n",
    "\n",
    "def prompt(essay_set):\n",
    "    if essay_set == 1:\n",
    "        return \"Write a letter to your local newspaper in which you state your opinion on the effects computers have on people\"\n",
    "    elif essay_set == 2:\n",
    "        return \"Write a persuasive essay to a newspaper reflecting your vies on censorship in libraries. Do you believe that certain materials, such as books, music, movies, magazines, etc., should be removed from the shelves if they are found offensive?\"\n",
    "    elif essay_set == 3:\n",
    "        return \"Write a response that explains how the features of the setting affect the cyclist\"\n",
    "    elif essay_set == 4:\n",
    "        return \"Write a response that explains why the author concludes the story with this paragraph\"\n",
    "    elif essay_set == 5:\n",
    "        return \"Describe the mood created by the author in the memoir\"\n",
    "    elif essay_set == 6:\n",
    "        return \"Based on the excerpt, describe the obstacles the builders of the Empire State Building faced in attempting to allow dirigibles to dock there\"\n",
    "    elif essay_set == 7:\n",
    "        return \"Do only one of the following: write a story about a time when you were patient OR write a story about a time when someone you know was patient OR write a story in your own way about patience\"\n",
    "    elif essay_set == 8:\n",
    "        return \"Tell a true story in which laughter was one element or part\"\n",
    "\n",
    "df['prompt'] = df['essay_set'].apply(prompt)\n",
    "\n",
    "def normalizeScores(essay_set, score):\n",
    "    if essay_set == 1:\n",
    "        return round((score/12), 2)*100\n",
    "    elif essay_set == 2:\n",
    "        return round((score/6), 2)*100\n",
    "    elif essay_set == 3:\n",
    "        return round((score/3), 2)*100\n",
    "    elif essay_set == 4:\n",
    "        return round((score/3), 2)*100\n",
    "    elif essay_set == 5:\n",
    "        return round((score/4), 2)*100\n",
    "    elif essay_set == 6:\n",
    "        return round((score/4), 2)*100\n",
    "    elif essay_set == 7:\n",
    "        return round((score/30), 2)*100\n",
    "    elif essay_set == 8:\n",
    "        return round((score/60), 2)*100\n",
    "    \n",
    "df['normalized_score'] = df.apply(lambda x : normalizeScores(x.essay_set, x.domain1_score), axis=1)\n",
    "df['normalized_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0         4.97\n",
       "1         5.14\n",
       "2        13.18\n",
       "3         8.05\n",
       "4         6.36\n",
       "         ...  \n",
       "12971     2.49\n",
       "12972     2.06\n",
       "12973     1.46\n",
       "12974     2.60\n",
       "12975     0.47\n",
       "Name: key_words_count, Length: 12976, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df['key_words_count'] = df.apply(lambda x: keyWords(x.prompt, x.cleaned_essay2_no_sw), axis=1)\n",
    "df['key_words_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentcount'] = df['essay'].apply(sentcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To feather\n",
    "df.to_feather('data/prelimData.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the test set\n",
    "\n",
    "test = pd.read_csv('~/Documents/GitHub/AutomatedEssayGrader/essaygrader/data/test_set.tsv', sep='\\t', encoding='ISO-8859-1')\n",
    "test.to_feather('data/test_set.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_feather('~/Documents/GitHub/AutomatedEssayGrader/essaygrader/data/test_set.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['essay_id', 'essay_set', 'essay', 'domain1_predictionid',\n",
       "       'domain2_predictionid'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean essays\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "test['cleaned_essay'] = test['essay'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  }
 ]
}